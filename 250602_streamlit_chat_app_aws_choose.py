import streamlit as st
import os
import json
# from langchain_google_genai import ChatGoogleGenerativeAI # Googleç”¨ã‚’å‰Šé™¤
from langchain_aws import ChatBedrock # Bedrockç”¨ã‚’è¿½åŠ 
from langchain.memory import ConversationBufferMemory
from langchain_community.tools import DuckDuckGoSearchResults
from langchain.agents import AgentExecutor, create_tool_calling_agent
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from botocore.exceptions import NoCredentialsError, ClientError, ProfileNotFound

# --- å®šæ•° ---
# Bedrockã§åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ã¨ãã®è¡¨ç¤ºå (é©å®œæ›´æ–°ã—ã¦ãã ã•ã„)
AVAILABLE_MODELS = {
    "Anthropic Claude 3 Sonnet": "anthropic.claude-3-sonnet-20240229-v1:0",
    "Anthropic Claude 3 Haiku": "anthropic.claude-3-haiku-20240307-v1:0",
    "Amazon Titan Text G1 - Express": "amazon.titan-text-express-v1",
    "Meta Llama 3 8B Instruct": "meta.llama3-8b-instruct-v1:0",
    "Cohere Command R": "cohere.command-r-v1:0",
}
DEFAULT_MODEL_DISPLAY_NAME = "Anthropic Claude 3 Sonnet"
DEFAULT_MODEL_API_NAME = AVAILABLE_MODELS[DEFAULT_MODEL_DISPLAY_NAME]
DEFAULT_MAX_TOKENS = 2048
DEFAULT_TEMPERATURE = 0.7 # æ¸©åº¦ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
DEFAULT_USE_SEARCH = True
DEFAULT_AWS_REGION = os.getenv("AWS_DEFAULT_REGION", "us-east-1") # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒªãƒ¼ã‚¸ãƒ§ãƒ³

# --- ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆã®åˆæœŸåŒ– ---
if "selected_model_name" not in st.session_state:
    st.session_state.selected_model_name = DEFAULT_MODEL_API_NAME
if "selected_max_tokens" not in st.session_state:
    st.session_state.selected_max_tokens = DEFAULT_MAX_TOKENS
if "selected_temperature" not in st.session_state: # æ¸©åº¦ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆ
    st.session_state.selected_temperature = DEFAULT_TEMPERATURE
if "selected_aws_region" not in st.session_state: # AWSãƒªãƒ¼ã‚¸ãƒ§ãƒ³ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆ
    st.session_state.selected_aws_region = DEFAULT_AWS_REGION
if "use_duckduckgo" not in st.session_state:
    st.session_state.use_duckduckgo = DEFAULT_USE_SEARCH

if "messages" not in st.session_state:
    st.session_state.messages = []
if "memory" not in st.session_state:
    st.session_state.memory = ConversationBufferMemory(
        memory_key="chat_history", return_messages=True
    )
if "search_url_history" not in st.session_state:
    st.session_state.search_url_history = []

# --- ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯é–¢æ•° ---
def on_settings_change():
    if "agent_executor" in st.session_state:
        del st.session_state.agent_executor

# --- LangChainã®ã‚³ã‚¢ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®åˆæœŸåŒ– ---

@st.cache_resource
def get_llm(model_id: str, max_tokens_from_ui: int, temperature_from_ui: float, region_name: str):
    try:
        # AWSèªè¨¼æƒ…å ±ã¯boto3ãŒç’°å¢ƒå¤‰æ•°ã‚„IAMãƒ­ãƒ¼ãƒ«ã‹ã‚‰è‡ªå‹•ã§èª­ã¿è¾¼ã‚€ã“ã¨ã‚’æœŸå¾…
        # ç‰¹å®šã®ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ç”¨ã™ã‚‹å ´åˆã¯ã€ChatBedrockã® credentials_profile_name ã‚’è¨­å®š
        # credentials_profile_name = os.getenv("AWS_PROFILE")

        model_kwargs = {}
        # ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ãƒŸãƒªãƒ¼ã«å¿œã˜ã¦ max_tokens ã®ã‚­ãƒ¼åã¨æ¸©åº¦è¨­å®šã‚’èª¿æ•´
        if "anthropic.claude" in model_id:
            model_kwargs["max_tokens_to_sample"] = max_tokens_from_ui
            # Claudeã®å ´åˆã€temperatureã¯ChatBedrockã®ã‚³ãƒ³ã‚¹ãƒˆãƒ©ã‚¯ã‚¿å¼•æ•°ã§è¨­å®š
        elif "amazon.titan" in model_id:
            model_kwargs["textGenerationConfig"] = {
                "maxTokenCount": max_tokens_from_ui,
                "temperature": temperature_from_ui, # Titanã¯ã“ã“ã§æ¸©åº¦è¨­å®š
                "stopSequences": [],
                "topP": 1.0,
            }
        elif "meta.llama" in model_id:
            model_kwargs["max_gen_len"] = max_tokens_from_ui
            # Llamaã®å ´åˆã€temperatureã¯ChatBedrockã®ã‚³ãƒ³ã‚¹ãƒˆãƒ©ã‚¯ã‚¿å¼•æ•°ã§è¨­å®š
        elif "cohere.command-r" in model_id:
            model_kwargs["max_tokens"] = max_tokens_from_ui
            # Cohereã®å ´åˆã€temperatureã¯ChatBedrockã®ã‚³ãƒ³ã‚¹ãƒˆãƒ©ã‚¯ã‚¿å¼•æ•°ã§è¨­å®š
        else:
            st.warning(f"ãƒ¢ãƒ‡ãƒ« {model_id} ã®ãŸã‚ã®ç‰¹å®šã® `max_tokens` ã‚­ãƒ¼ãŒä¸æ˜ã§ã™ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã‚’è©¦ã¿ã¾ã™ãŒã€å‹•ä½œã—ãªã„å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚")
            # å¿…è¦ã§ã‚ã‚Œã°ã“ã“ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚„ã‚¨ãƒ©ãƒ¼å‡¦ç†ã‚’è¿½åŠ 

        llm = ChatBedrock(
            region_name=region_name,
            # credentials_profile_name=credentials_profile_name, # ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«åã§èªè¨¼ã™ã‚‹å ´åˆ
            model_id=model_id,
            model_kwargs=model_kwargs if model_kwargs else None,
            temperature=temperature_from_ui if "amazon.titan" not in model_id else None, # Titanä»¥å¤–ã¯ã“ã“ã§è¨­å®š
            # streaming=True, # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å¿œç­”ãŒå¿…è¦ãªå ´åˆ
        )
        return llm

    except (NoCredentialsError, ProfileNotFound):
        st.error("AWSèªè¨¼æƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚AWS CLIãŒè¨­å®šã•ã‚Œã¦ã„ã‚‹ã‹ã€ç’°å¢ƒå¤‰æ•° (AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, AWS_SESSION_TOKEN (ã‚ªãƒ—ã‚·ãƒ§ãƒ³)) ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚ã¾ãŸã¯ã€æœ‰åŠ¹ãªAWSãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¨­å®šã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        st.stop()
    except ClientError as e:
        error_code = e.response.get("Error", {}).get("Code")
        if error_code == "AccessDeniedException" or "AccessDenied" in str(e) :
            st.error(f"Bedrockã¸ã®ã‚¢ã‚¯ã‚»ã‚¹ãŒæ‹’å¦ã•ã‚Œã¾ã—ãŸã€‚IAMæ¨©é™ã¨ãƒ¢ãƒ‡ãƒ«ã‚¢ã‚¯ã‚»ã‚¹ãŒæœ‰åŠ¹ã«ãªã£ã¦ã„ã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„: {e}")
        elif error_code == "ValidationException" and "modelId" in str(e):
            st.error(f"æŒ‡å®šã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ID '{model_id}' ãŒãƒªãƒ¼ã‚¸ãƒ§ãƒ³ '{region_name}' ã§ç„¡åŠ¹ã‹ã€ã‚¢ã‚¯ã‚»ã‚¹æ¨©ãŒã‚ã‚Šã¾ã›ã‚“ã€‚Bedrockã‚³ãƒ³ã‚½ãƒ¼ãƒ«ã§ãƒ¢ãƒ‡ãƒ«ã‚¢ã‚¯ã‚»ã‚¹ã‚’æœ‰åŠ¹ã«ã—ã¦ãã ã•ã„ã€‚ã‚¨ãƒ©ãƒ¼: {e}")
        else:
            st.error(f"AWS Bedrock APIã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        st.stop()
    except Exception as e:
        st.error(f"LLM ({model_id}) ã®åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        st.stop()

@st.cache_resource
def get_tools(enable_search: bool):
    if enable_search:
        search_tool = DuckDuckGoSearchResults(name="duckduckgo_results_json", max_results=3)
        return [search_tool]
    return []

def get_agent_executor(llm, tools):
    if "agent_executor" not in st.session_state:
        system_message_parts = ["ã‚ãªãŸã¯è¦ªåˆ‡ã§å½¹ç«‹ã¤ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚"]
        if tools:
            system_message_parts.append("å¿…è¦ã«å¿œã˜ã¦DuckDuckGoã‚’ä½¿ã£ã¦Webæ¤œç´¢ã‚’è¡Œã„ã€æœ€æ–°æƒ…å ±ã‚„ç‰¹å®šã®æƒ…å ±ã‚’èª¿ã¹ã¦ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«ç­”ãˆã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚æ¤œç´¢çµæœã‚’åˆ©ç”¨ã—ãŸå ´åˆã¯ã€ãã®æƒ…å ±æºã«ã¤ã„ã¦è§¦ã‚Œã‚‹ã‚ˆã†ã«ã—ã¦ãã ã•ã„ã€‚")
        else:
            system_message_parts.append("Webæ¤œç´¢æ©Ÿèƒ½ã¯ç¾åœ¨ã‚ªãƒ•ã«ãªã£ã¦ã„ã¾ã™ã€‚")
        
        final_system_message = " ".join(system_message_parts)

        prompt = ChatPromptTemplate.from_messages(
            [
                ("system", final_system_message),
                MessagesPlaceholder(variable_name="chat_history", optional=True),
                ("user", "{input}"),
                MessagesPlaceholder(variable_name="agent_scratchpad"),
            ]
        )
        agent = create_tool_calling_agent(llm, tools, prompt)
        st.session_state.agent_executor = AgentExecutor(
            agent=agent,
            tools=tools,
            verbose=True,
            handle_parsing_errors=True,
            return_intermediate_steps=True
        )
    return st.session_state.agent_executor

# --- Streamlit ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®UIã¨ãƒ­ã‚¸ãƒƒã‚¯ ---
st.set_page_config(page_title="Bedrock Webæ¤œç´¢ãƒãƒ£ãƒƒãƒˆ (è¨­å®šå¯èƒ½)", layout="wide")
st.title("AWS Bedrock Webæ¤œç´¢ãƒãƒ£ãƒƒãƒˆ ğŸŒ (è¨­å®šå¯èƒ½)")

# --- ã‚µã‚¤ãƒ‰ãƒãƒ¼ ---
with st.sidebar:
    st.header("LLMã¨è¨€èªãƒ¢ãƒ‡ãƒ«è¨­å®š")

    # AWSãƒªãƒ¼ã‚¸ãƒ§ãƒ³é¸æŠ
    # ä¸€èˆ¬çš„ãªBedrockãƒªãƒ¼ã‚¸ãƒ§ãƒ³ãƒªã‚¹ãƒˆ (å¿…è¦ã«å¿œã˜ã¦æ›´æ–°)
    common_aws_regions = [
        "us-east-1", "us-west-2", "ap-northeast-1", "ap-southeast-1", 
        "eu-central-1", "eu-west-1", "eu-west-2"
    ]
    try:
        default_region_index = common_aws_regions.index(st.session_state.selected_aws_region)
    except ValueError:
        default_region_index = 0 # è¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯æœ€åˆã®ãƒªãƒ¼ã‚¸ãƒ§ãƒ³ã‚’ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã«
        st.session_state.selected_aws_region = common_aws_regions[default_region_index]

    selected_region_name = st.selectbox(
        "AWS ãƒªãƒ¼ã‚¸ãƒ§ãƒ³:",
        options=common_aws_regions,
        index=default_region_index,
        on_change=on_settings_change,
        key="sb_aws_region"
    )
    st.session_state.selected_aws_region = selected_region_name

    # LLMãƒ¢ãƒ‡ãƒ«é¸æŠ
    current_model_display_name = DEFAULT_MODEL_DISPLAY_NAME
    for display_name, api_name in AVAILABLE_MODELS.items():
        if api_name == st.session_state.selected_model_name:
            current_model_display_name = display_name
            break
    
    selected_display_name = st.selectbox(
        "LLMãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠ:",
        options=list(AVAILABLE_MODELS.keys()),
        index=list(AVAILABLE_MODELS.keys()).index(current_model_display_name),
        on_change=on_settings_change,
        key="sb_model_display_name"
    )
    st.session_state.selected_model_name = AVAILABLE_MODELS[selected_display_name]

    # æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°
    st.session_state.selected_max_tokens = st.number_input(
        "æœ€å¤§å‡ºåŠ›ãƒˆãƒ¼ã‚¯ãƒ³æ•°:",
        min_value=256,
        max_value=100000, # ãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã‚Šä¸Šé™ãŒå¤§ããç•°ãªã‚‹ãŸã‚ã€é«˜ã‚ã«è¨­å®š (Claude 3 Sonnet ã¯200K context)
        value=st.session_state.selected_max_tokens,
        step=128,
        on_change=on_settings_change,
        key="ni_max_tokens"
    )

    # æ¸©åº¦è¨­å®š
    st.session_state.selected_temperature = st.slider(
        "Temperature (å‡ºåŠ›ã®å¤šæ§˜æ€§):",
        min_value=0.0,
        max_value=1.0, # Titanã¯2.0ã¾ã§ãªã©ãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã‚‹ãŒã€ä¸€èˆ¬çš„ã«ã¯0.0-1.0
        value=st.session_state.selected_temperature,
        step=0.05,
        on_change=on_settings_change,
        key="slider_temperature"
    )

    st.markdown("---")
    st.header("ãƒ„ãƒ¼ãƒ«è¨­å®š")
    st.session_state.use_duckduckgo = st.toggle(
        "DuckDuckGo Webæ¤œç´¢ã‚’æœ‰åŠ¹ã«ã™ã‚‹",
        value=st.session_state.use_duckduckgo,
        on_change=on_settings_change,
        key="toggle_use_search"
    )

    st.markdown("---")
    st.header("ä¼šè©±æ“ä½œ")
    if st.button("ä¼šè©±å±¥æ­´ã‚’ãƒªã‚»ãƒƒãƒˆ", key="reset_chat"):
        st.session_state.messages = []
        st.session_state.search_url_history = []
        if "agent_executor" in st.session_state:
            del st.session_state.agent_executor
        if "memory" in st.session_state:
            del st.session_state.memory
            st.session_state.memory = ConversationBufferMemory(
                memory_key="chat_history", return_messages=True
            )
        st.rerun()

    st.markdown("---")
    st.subheader("æ¤œç´¢ã•ã‚ŒãŸURLå±¥æ­´")
    # (å¤‰æ›´ãªã—)

    st.markdown("---")
    st.subheader("AWSèªè¨¼ã«ã¤ã„ã¦")
    st.caption(
        "ã“ã®ã‚¢ãƒ—ãƒªã¯AWSèªè¨¼æƒ…å ±ã‚’åˆ©ç”¨ã—ã¾ã™ã€‚ä»¥ä¸‹ã®ã„ãšã‚Œã‹ã®æ–¹æ³•ã§èªè¨¼æƒ…å ±ã‚’è¨­å®šã—ã¦ãã ã•ã„:\n"
        "1. ç’°å¢ƒå¤‰æ•° (AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, AWS_SESSION_TOKEN (ã‚ªãƒ—ã‚·ãƒ§ãƒ³), AWS_DEFAULT_REGION)\n"
        "2. AWS CLIã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ« (~/.aws/credentials ã¨ ~/.aws/config)\n"
        "3. (EC2/ECS/Lambdaãªã©) IAMãƒ­ãƒ¼ãƒ«"
    )
    st.markdown("[AWSèªè¨¼æƒ…å ±ã®è¨­å®šè©³ç´°](https://boto3.amazonaws.com/v1/documentation/api/latest/guide/credentials.html)")


    st.markdown("---")
    st.subheader("ãƒ‡ãƒãƒƒã‚°æƒ…å ±")
    # (å¤‰æ›´ãªã—)

# --- ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ ---
current_llm = get_llm(
    st.session_state.selected_model_name,
    st.session_state.selected_max_tokens,
    st.session_state.selected_temperature,
    st.session_state.selected_aws_region
)
current_tools = get_tools(st.session_state.use_duckduckgo)
agent_executor = get_agent_executor(current_llm, current_tools)

caption_model_display_name = [k for k, v in AVAILABLE_MODELS.items() if v == st.session_state.selected_model_name][0]
st.caption(f"LLM: Bedrock ({caption_model_display_name} in {st.session_state.selected_aws_region}), Search: DuckDuckGo ({'ON' if st.session_state.use_duckduckgo else 'OFF'})")

# ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã®è¡¨ç¤º (å¤‰æ›´ãªã—)
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›å‡¦ç† (URLæŠ½å‡ºãƒ­ã‚¸ãƒƒã‚¯ã®ã‚¬ãƒ¼ãƒ‰ä¿®æ­£ã‚’å«ã‚€)
if user_prompt := st.chat_input("ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„..."):
    st.session_state.messages.append({"role": "user", "content": user_prompt})
    with st.chat_message("user"):
        st.markdown(user_prompt)

    with st.chat_message("assistant"):
        try:
            with st.spinner("AIãŒè€ƒãˆã¦ã„ã¾ã™..."):
                chat_history_messages = st.session_state.memory.chat_memory.messages
                response_data = agent_executor.invoke(
                    {
                        "input": user_prompt,
                        "chat_history": chat_history_messages
                    }
                )
                ai_response = response_data.get('output', "ç”³ã—è¨³ã‚ã‚Šã¾ã›ã‚“ã€å¿œç­”ã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")
                intermediate_steps = response_data.get('intermediate_steps', [])

                urls_found_this_turn = []
                if st.session_state.use_duckduckgo:
                    for step in intermediate_steps:
                        action, observation = step
                        # action ãŒ AgentAction ã‹ã¤ tool ãŒ duckduckgo_results_json ã§ã‚ã‚‹ã“ã¨ã‚’ç¢ºèª
                        if hasattr(action, 'tool') and action.tool == "duckduckgo_results_json":
                            if isinstance(observation, str):
                                try:
                                    results_list = json.loads(observation)
                                    for res_item in results_list:
                                        if isinstance(res_item, dict) and "link" in res_item:
                                            urls_found_this_turn.append(res_item["link"])
                                except json.JSONDecodeError:
                                    st.warning(f"æ¤œç´¢çµæœã®JSONè§£æã«å¤±æ•—ã—ã¾ã—ãŸ: {observation[:200]}...")
                            elif isinstance(observation, list):
                                for res_item in observation:
                                    if isinstance(res_item, dict) and "link" in res_item:
                                        urls_found_this_turn.append(res_item["link"])

                for url in urls_found_this_turn:
                    if url not in st.session_state.search_url_history:
                        st.session_state.search_url_history.append(url)

            st.markdown(ai_response)
            st.session_state.messages.append({"role": "assistant", "content": ai_response})
            st.session_state.memory.chat_memory.add_user_message(user_prompt)
            st.session_state.memory.chat_memory.add_ai_message(ai_response)

        except Exception as e:
            error_message = f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}"
            st.error(error_message)
            st.session_state.messages.append({"role": "assistant", "content": error_message})