import streamlit as st
import os
import json
from botocore.config import Config # Boto3ã®ã‚³ãƒ³ãƒ•ã‚£ã‚°ç”¨
import boto3 # AWS SDK
from langchain_aws import ChatBedrock # Bedrocké€£æºç”¨
from langchain.memory import ConversationBufferMemory
from langchain_community.tools import DuckDuckGoSearchResults
from langchain.agents import AgentExecutor, create_tool_calling_agent
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder

# --- åˆæœŸè¨­å®š ---
# Bedrockãƒ¢ãƒ‡ãƒ«ã®ãƒªã‚¹ãƒˆ (è¡¨ç¤ºå: ãƒ¢ãƒ‡ãƒ«ID)
AVAILABLE_BEDROCK_MODELS = {
    "Claude 3 Haiku (Anthropic)": "anthropic.claude-3-haiku-20240307-v1:0",
    "Claude 3 Sonnet (Anthropic)": "anthropic.claude-3-sonnet-20240229-v1:0",
    "Claude 3 Opus (Anthropic)": "anthropic.claude-3-opus-20240229-v1:0", # é«˜æ€§èƒ½ã ãŒé«˜ä¾¡ãƒ»ãƒªãƒ¼ã‚¸ãƒ§ãƒ³æ³¨æ„
    "Llama 3 8B Instruct (Meta)": "meta.llama3-8b-instruct-v1:0",
    "Llama 3 70B Instruct (Meta)": "meta.llama3-70b-instruct-v1:0", # é«˜æ€§èƒ½ã ãŒãƒªãƒ¼ã‚¸ãƒ§ãƒ³æ³¨æ„
    # "Amazon Titan Text G1 - Express": "amazon.titan-text-express-v1", # å¿…è¦ã«å¿œã˜ã¦è¿½åŠ 
    # "Mistral Large": "mistral.mistral-large-2402-v1:0" # ãƒªãƒ¼ã‚¸ãƒ§ãƒ³æ³¨æ„
}
DEFAULT_BEDROCK_MODEL_DISPLAY_NAME = "Claude 3 Haiku (Anthropic)" # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ¢ãƒ‡ãƒ«
AWS_BEDROCK_REGION = "us-east-1"  # Bedrockã‚’åˆ©ç”¨ã™ã‚‹AWSãƒªãƒ¼ã‚¸ãƒ§ãƒ³ (é©å®œå¤‰æ›´ã—ã¦ãã ã•ã„)

# --- ã‚»ãƒƒã‚·ãƒ§ãƒ³ãƒªã‚»ãƒƒãƒˆé–¢æ•° ---
def reset_chat_and_agent_state():
    st.session_state.messages = []
    st.session_state.search_url_history = []
    keys_to_delete_on_model_change = ["memory", "agent_executor_instance", "cached_llm_instance"]
    for key in keys_to_delete_on_model_change:
        if key in st.session_state:
            del st.session_state[key]
    # @st.cache_resourceã§ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã•ã‚ŒãŸé–¢æ•°ã‚’ã‚¯ãƒªã‚¢ (å¼•æ•°å¤‰æ›´ã§å†å®Ÿè¡Œã‚’ä¿ƒã™)

# --- LangChainã®ã‚³ã‚¢ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®åˆæœŸåŒ– ---

@st.cache_resource # ãƒªãƒ¼ã‚¸ãƒ§ãƒ³ãŒå¤‰ã‚ã‚‹ã“ã¨ã‚‚è€ƒæ…®ã—ã¦å¼•æ•°ã«å«ã‚ã‚‹
def get_boto_client(_region_name: str): # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼ã®ãŸã‚å¼•æ•°åã«ã‚¢ãƒ³ãƒ€ãƒ¼ã‚¹ã‚³ã‚¢
    """ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºã•ã‚ŒãŸBoto3ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’å–å¾—"""
    boto_config = Config(
        read_timeout=90,    # èª­ã¿å–ã‚Šã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚’90ç§’ã«è¨­å®š
        connect_timeout=60, # æ¥ç¶šã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚’60ç§’ã«è¨­å®š
        retries={'max_attempts': 3} # ãƒªãƒˆãƒ©ã‚¤å›æ•°ã‚’3å›ã«è¨­å®š
    )
    return boto3.client(
        service_name="bedrock-runtime",
        region_name=_region_name,
        config=boto_config
    )

@st.cache_resource
def get_llm(model_id: str, region_name: str):
    """é¸æŠã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«IDã¨ãƒªãƒ¼ã‚¸ãƒ§ãƒ³ã«åŸºã¥ã„ã¦ChatBedrock LLMã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’å–å¾—"""
    st.write(f"DEBUG: Initializing Bedrock LLM: {model_id} in {region_name}") # ãƒ‡ãƒãƒƒã‚°ç”¨
    try:
        bedrock_boto_client = get_boto_client(region_name)

        # ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ãƒŸãƒªãƒ¼ã«å¿œã˜ã¦model_kwargsã‚’èª¿æ•´
        model_kwargs = {"temperature": 0.7}
        if "anthropic.claude" in model_id:
            model_kwargs["max_tokens_to_sample"] = 2048
            # Claude v3ã¯max_tokensã‚’ç›´æ¥æŒ‡å®šã§ãã‚‹ã‚ˆã†ã«ãªã£ãŸ (LangChainã®ChatBedrockãŒå¯¾å¿œã—ã¦ã„ã‚‹ã‹ç¢ºèª)
            # model_kwargs["max_tokens"] = 2048 # Bedrock APIã®Claude3ã§ã¯ã“ã¡ã‚‰ãŒæ¨å¥¨
        elif "meta.llama" in model_id:
            model_kwargs["max_gen_len"] = 2048
            model_kwargs["top_p"] = 0.9
        elif "amazon.titan" in model_id:
            model_kwargs["textGenerationConfig"] = {"maxTokenCount": 2048, "temperature":0.7}
        elif "mistral" in model_id:
            model_kwargs["max_tokens"] = 2048

        llm = ChatBedrock(
            client=bedrock_boto_client, # ã‚«ã‚¹ã‚¿ãƒ Boto3ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’ä½¿ç”¨
            model_id=model_id,
            model_kwargs=model_kwargs,
            # streaming=True, # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãŒå¿…è¦ãªå ´åˆ
        )
        return llm
    except Exception as e:
        st.error(f"LLM (Bedrock: {model_id}) ã®åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        st.markdown("""
            **è€ƒãˆã‚‰ã‚Œã‚‹åŸå› ã¨å¯¾å‡¦æ³•:**
            - AWSèªè¨¼æƒ…å ±ï¼ˆç’°å¢ƒå¤‰æ•°ã€`~/.aws/credentials`ã€IAMãƒ­ãƒ¼ãƒ«ãªã©ï¼‰ãŒæ­£ã—ãè¨­å®šã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„ã€‚
            - æŒ‡å®šã—ãŸãƒ¢ãƒ‡ãƒ«IDãŒã€é¸æŠã—ãŸAWSãƒªãƒ¼ã‚¸ãƒ§ãƒ³ (`{region_name}`) ã§åˆ©ç”¨å¯èƒ½ã‹ç¢ºèªã—ã¦ãã ã•ã„ã€‚
            - Bedrockã‚µãƒ¼ãƒ“ã‚¹ã¸ã®ã‚¢ã‚¯ã‚»ã‚¹æ¨©é™ãŒIAMãƒ¦ãƒ¼ã‚¶ãƒ¼/ãƒ­ãƒ¼ãƒ«ã«ä»˜ä¸ã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„ã€‚
        """)
        st.stop()

@st.cache_resource
def get_tools():
    search_tool = DuckDuckGoSearchResults(name="duckduckgo_results_json", max_results=3)
    return [search_tool]

@st.cache_resource
def get_cached_agent_executor(_llm_model_id_for_cache_key: str, _aws_region_for_cache_key: str):
    llm = get_llm(_llm_model_id_for_cache_key, _aws_region_for_cache_key)
    tools = get_tools()
    prompt = ChatPromptTemplate.from_messages(
        [
            ("system", "ã‚ãªãŸã¯è¦ªåˆ‡ã§å½¹ç«‹ã¤ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚å¿…è¦ã«å¿œã˜ã¦DuckDuckGoã‚’ä½¿ã£ã¦Webæ¤œç´¢ã‚’è¡Œã„ã€æœ€æ–°æƒ…å ±ã‚„ç‰¹å®šã®æƒ…å ±ã‚’èª¿ã¹ã¦ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«ç­”ãˆã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚æ¤œç´¢çµæœã‚’åˆ©ç”¨ã—ãŸå ´åˆã¯ã€ãã®æƒ…å ±æºã«ã¤ã„ã¦è§¦ã‚Œã‚‹ã‚ˆã†ã«ã—ã¦ãã ã•ã„ã€‚"),
            MessagesPlaceholder(variable_name="chat_history", optional=True),
            ("user", "{input}"),
            MessagesPlaceholder(variable_name="agent_scratchpad"),
        ]
    )
    agent = create_tool_calling_agent(llm, tools, prompt)
    agent_executor = AgentExecutor(
        agent=agent,
        tools=tools,
        verbose=True,
        handle_parsing_errors=True,
        return_intermediate_steps=True
    )
    return agent_executor

# --- Streamlit ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®UIã¨ãƒ­ã‚¸ãƒƒã‚¯ ---

st.set_page_config(page_title="Bedrock LLMé¸æŠå¼ Webæ¤œç´¢ãƒãƒ£ãƒƒãƒˆ", layout="wide")
st.title("Bedrock LLMé¸æŠå¼ Webæ¤œç´¢ãƒãƒ£ãƒƒãƒˆ ğŸ¤–â˜ï¸")

# --- ã‚µã‚¤ãƒ‰ãƒãƒ¼ã§ã®ãƒ¢ãƒ‡ãƒ«é¸æŠã¨è¨­å®š ---
with st.sidebar:
    st.header("è¨­å®š")

    # ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆã®åˆæœŸåŒ– (é¸æŠãƒ¢ãƒ‡ãƒ«ã¨ãƒªãƒ¼ã‚¸ãƒ§ãƒ³)
    if "selected_model_display_name" not in st.session_state:
        st.session_state.selected_model_display_name = DEFAULT_BEDROCK_MODEL_DISPLAY_NAME
    if "selected_bedrock_model_id" not in st.session_state:
        st.session_state.selected_bedrock_model_id = AVAILABLE_BEDROCK_MODELS[DEFAULT_BEDROCK_MODEL_DISPLAY_NAME]
    # ãƒªãƒ¼ã‚¸ãƒ§ãƒ³ã¯å›ºå®šã¨ã™ã‚‹ãŒã€ã‚‚ã—é¸æŠå¼ã«ã—ãŸã„å ´åˆã¯ã“ã“ã«è¿½åŠ 
    st.session_state.aws_bedrock_region = AWS_BEDROCK_REGION


    new_selected_model_display_name = st.selectbox(
        "LLMãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠ (AWS Bedrock):",
        options=list(AVAILABLE_BEDROCK_MODELS.keys()),
        index=list(AVAILABLE_BEDROCK_MODELS.keys()).index(st.session_state.selected_model_display_name),
        key="bedrock_model_selector_key"
    )

    if new_selected_model_display_name != st.session_state.selected_model_display_name:
        st.session_state.selected_model_display_name = new_selected_model_display_name
        st.session_state.selected_bedrock_model_id = AVAILABLE_BEDROCK_MODELS[new_selected_model_display_name]
        reset_chat_and_agent_state()
        st.rerun()

st.caption(f"Powered by LangChain & Streamlit, using: {st.session_state.selected_bedrock_model_id} in {st.session_state.aws_bedrock_region}")

# LLMã¨AgentExecutorã®æº–å‚™
agent_executor = get_cached_agent_executor(st.session_state.selected_bedrock_model_id, st.session_state.aws_bedrock_region)

# ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆã®åˆæœŸåŒ– (ä¼šè©±å±¥æ­´ã€ãƒ¡ãƒ¢ãƒªã€URLå±¥æ­´)
if "messages" not in st.session_state:
    st.session_state.messages = []
if "memory" not in st.session_state:
    st.session_state.memory = ConversationBufferMemory(
        memory_key="chat_history", return_messages=True
    )
if "search_url_history" not in st.session_state:
    st.session_state.search_url_history = []

# ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã®è¡¨ç¤º
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›ã®å—ä»˜ã¨å‡¦ç†
if user_prompt := st.chat_input("ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„..."):
    st.session_state.messages.append({"role": "user", "content": user_prompt})
    with st.chat_message("user"):
        st.markdown(user_prompt)

    with st.chat_message("assistant"):
        try:
            with st.spinner(f"{st.session_state.selected_bedrock_model_id.split('.')[1]} ãŒè€ƒãˆã¦ã„ã¾ã™..."): # ãƒ¢ãƒ‡ãƒ«åéƒ¨åˆ†ã‚’çŸ­ç¸®è¡¨ç¤º
                chat_history_messages = st.session_state.memory.chat_memory.messages
                response_data = agent_executor.invoke(
                    {"input": user_prompt, "chat_history": chat_history_messages}
                )
                ai_response = response_data.get('output', "ç”³ã—è¨³ã‚ã‚Šã¾ã›ã‚“ã€å¿œç­”ã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")
                intermediate_steps = response_data.get('intermediate_steps', [])

                urls_found_this_turn = []
                for step in intermediate_steps:
                    action, observation = step
                    if action.tool == "duckduckgo_results_json": # ãƒ„ãƒ¼ãƒ«åã‚’ç¢ºèª
                        if isinstance(observation, str):
                            try:
                                results_list = json.loads(observation)
                                for res_item in results_list:
                                    if isinstance(res_item, dict) and "link" in res_item:
                                        urls_found_this_turn.append(res_item["link"])
                            except json.JSONDecodeError:
                                st.warning(f"æ¤œç´¢çµæœã®JSONè§£æã«å¤±æ•—: {observation[:100]}...")
                        elif isinstance(observation, list):
                             for res_item in observation:
                                if isinstance(res_item, dict) and "link" in res_item:
                                    urls_found_this_turn.append(res_item["link"])
                for url in urls_found_this_turn:
                    if url not in st.session_state.search_url_history:
                        st.session_state.search_url_history.append(url)

            st.markdown(ai_response)
            st.session_state.messages.append({"role": "assistant", "content": ai_response})
            st.session_state.memory.chat_memory.add_user_message(user_prompt)
            st.session_state.memory.chat_memory.add_ai_message(ai_response)

        except Exception as e:
            error_message = f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}"
            st.error(error_message)
            st.session_state.messages.append({"role": "assistant", "content": error_message})

# --- ã‚µã‚¤ãƒ‰ãƒãƒ¼ (ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã¨ãƒ‡ãƒãƒƒã‚°æƒ…å ±) ---
with st.sidebar:
    # ... (ãƒ¢ãƒ‡ãƒ«é¸æŠã¯ä¸Šã«ç§»å‹•) ...
    st.markdown("---")
    if st.button("ä¼šè©±å±¥æ­´ã‚’ãƒªã‚»ãƒƒãƒˆ", key="reset_chat_button_bedrock"):
        reset_chat_and_agent_state()
        st.rerun()

    st.markdown("---")
    st.subheader("æ¤œç´¢ã•ã‚ŒãŸURLå±¥æ­´")
    if st.session_state.get("search_url_history"):
        for i, url in enumerate(reversed(st.session_state.search_url_history)):
            try:
                domain = url.split('//')[-1].split('/')[0]
            except:
                domain = "ä¸æ˜ãªãƒ‰ãƒ¡ã‚¤ãƒ³"
            st.markdown(f"{len(st.session_state.search_url_history) - i}. [{domain}]({url})")
    else:
        st.caption("ã¾ã æ¤œç´¢ã¯è¡Œã‚ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")

    st.markdown("---")
    st.subheader("AWSèªè¨¼ã«ã¤ã„ã¦")
    st.caption(f"ã“ã®ã‚¢ãƒ—ãƒªã¯ã€è¨­å®šæ¸ˆã¿ã®AWSèªè¨¼æƒ…å ±ã¨ãƒªãƒ¼ã‚¸ãƒ§ãƒ³ ({st.session_state.aws_bedrock_region}) ã‚’ä½¿ç”¨ã—ã¦AWS Bedrockã«æ¥ç¶šã—ã¾ã™ã€‚")
    st.markdown("""
        AWSèªè¨¼æƒ…å ±ãŒæ­£ã—ãè¨­å®šã•ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚ä¸€èˆ¬çš„ãªè¨­å®šæ–¹æ³•ã¯ä»¥ä¸‹ã®é€šã‚Šã§ã™:
        - IAMãƒ­ãƒ¼ãƒ« (EC2, Lambda, ECSãªã©ã§å®Ÿè¡Œã™ã‚‹å ´åˆ)
        - ç’°å¢ƒå¤‰æ•° (`AWS_ACCESS_KEY_ID`, `AWS_SECRET_ACCESS_KEY`, `AWS_SESSION_TOKEN`)
        - AWSèªè¨¼æƒ…å ±ãƒ•ã‚¡ã‚¤ãƒ« (`~/.aws/credentials` ãŠã‚ˆã³ `~/.aws/config`)
    """)

    st.markdown("---")
    st.subheader("ãƒ‡ãƒãƒƒã‚°æƒ…å ±")
    if st.checkbox("ä¼šè©±ãƒ¡ãƒ¢ãƒªã‚’è¡¨ç¤º (LangChain)"):
        if "memory" in st.session_state:
            st.write(st.session_state.memory.chat_memory.messages)
        else:
            st.caption("ãƒ¡ãƒ¢ãƒªã¯ã¾ã åˆæœŸåŒ–ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")