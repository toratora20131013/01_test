{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1d0696",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "環境変数からGemini APIキーを読み込みました。\n",
      "\n",
      "--- 設定内容 ---\n",
      "問題の内容: 導電性高分子コンデンサの信頼性悪化\n",
      "層数: 3\n",
      "初期観点: ['製造プロセス', '材料', '使用環境']\n",
      "各ノードの子要素の最大数: 3\n",
      "出力ファイル名: 導電性高分子コンデンサの信頼性悪化_mindmap_gemini.md\n",
      "使用モデル: gemini-2.0-flash\n",
      "Temperature: 0.2\n",
      "------------------\n"
     ]
    }
   ],
   "source": [
    "# 必要なライブラリのインポート\n",
    "import os\n",
    "import json\n",
    "import getpass\n",
    "from typing import List # Python標準の型ヒントを使用\n",
    "\n",
    "# LangChain関連のインポート\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "# from langchain_core.messages import SystemMessage, HumanMessage # ChatPromptTemplateで代替\n",
    "\n",
    "# --- ユーザー設定項目 ---\n",
    "\n",
    "# Google Gemini APIキーの入力\n",
    "api_key_env = os.getenv(\"GEMINI_API_KEY\")\n",
    "if api_key_env:\n",
    "    GEMINI_API_KEY = api_key_env\n",
    "    print(\"環境変数からGemini APIキーを読み込みました。\")\n",
    "else:\n",
    "    print(\"Gemini APIキーを入力してください（入力内容は表示されません）。\")\n",
    "    GEMINI_API_KEY = getpass.getpass()\n",
    "\n",
    "PROBLEM_TEXT = \"XXX\"\n",
    "MAX_DEPTH = 3\n",
    "INITIAL_VIEWPOINTS = [\"AAA\", \"BBB\", \"CCC\"]\n",
    "MAX_CHILDREN_PER_NODE = 3\n",
    "safe_problem_text = PROBLEM_TEXT.replace(' ', '_').replace('/', '_').replace('\\\\', '_')\n",
    "OUTPUT_FILENAME = f\"{safe_problem_text}_mindmap_gemini.md\"\n",
    "GEMINI_MODEL = \"gemini-2.0-flash\" # ユーザー指定の gemini-2.0-flash が利用できない場合を考慮し、より一般的な最新モデルに変更\n",
    "TEMPERATURE = 0.2\n",
    "\n",
    "print(f\"\\n--- 設定内容 ---\")\n",
    "print(f\"問題の内容: {PROBLEM_TEXT}\")\n",
    "print(f\"層数: {MAX_DEPTH}\")\n",
    "print(f\"初期観点: {INITIAL_VIEWPOINTS if INITIAL_VIEWPOINTS else 'なし'}\")\n",
    "print(f\"各ノードの子要素の最大数: {MAX_CHILDREN_PER_NODE}\")\n",
    "print(f\"出力ファイル名: {OUTPUT_FILENAME}\")\n",
    "print(f\"使用モデル: {GEMINI_MODEL}\") # ユーザー指定が gemini-2.0-flash の場合はここを元に戻してください\n",
    "print(f\"Temperature: {TEMPERATURE}\")\n",
    "print(f\"------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bbf972d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain LLM (gemini-2.0-flash) が正常に初期化されました。\n"
     ]
    }
   ],
   "source": [
    "# LangChain LLM (Gemini) の初期化\n",
    "llm = None # 初期化失敗に備える\n",
    "if not GEMINI_API_KEY:\n",
    "    print(\"エラー: Gemini APIキーが設定されていません。LLMを初期化できません。\")\n",
    "else:\n",
    "    try:\n",
    "        llm = ChatGoogleGenerativeAI(\n",
    "            model=GEMINI_MODEL,\n",
    "            google_api_key=GEMINI_API_KEY,\n",
    "            temperature=TEMPERATURE,\n",
    "            # GeminiはSystemMessageをネイティブサポートする場合としない場合がある。\n",
    "            # LangChainが内部でHumanMessageに変換することがある。\n",
    "            # convert_system_message_to_human=True # 必要に応じて有効化\n",
    "        )\n",
    "        # 簡単なテスト呼び出し (APIキーとモデルの有効性を確認)\n",
    "        # test_response = llm.invoke(\"こんにちは、テストです。\")\n",
    "        # print(f\"LLMテスト応答: {test_response.content[:50]}...\") # 応答の先頭を表示\n",
    "        print(f\"LangChain LLM ({GEMINI_MODEL}) が正常に初期化されました。\")\n",
    "    except Exception as e:\n",
    "        print(f\"LLMの初期化に失敗しました: {e}\")\n",
    "        print(\"考えられる原因: APIキーが無効、指定されたモデル名が利用できない、必要なライブラリが不足しているなど。\")\n",
    "        print(f\"指定されたモデル '{GEMINI_MODEL}' が利用可能か確認してください。代替として 'gemini-1.5-flash-latest' や 'gemini-pro' などがあります。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f153e938",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_factors_from_langchain_gemini(\n",
    "    parent_factor: str,\n",
    "    context_problem: str,\n",
    "    target_depth: int,\n",
    "    max_depth_of_mindmap: int,\n",
    "    llm_model, # LangChain llm instance\n",
    "    viewpoint_context: str = None,\n",
    "    max_children: int = 3\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    LangChain経由でGemini APIを使用し、指定された親要素の下位要因を生成する。\n",
    "\n",
    "    Args:\n",
    "        parent_factor: 親要素の名前。\n",
    "        context_problem: マインドマップ全体の主題。\n",
    "        target_depth: 生成する要因が属する階層の深さ。\n",
    "        max_depth_of_mindmap: マインドマップ全体の最大の深さ。\n",
    "        llm_model: LangChain LLMインスタンス。\n",
    "        viewpoint_context: 親要素が属する主要な観点。\n",
    "        max_children: 生成を試みる子要素の最大数。\n",
    "\n",
    "    Returns:\n",
    "        生成された要因名 (文字列) のリスト。エラー時や要因が見つからない場合は空リスト。\n",
    "    \"\"\"\n",
    "    if not llm_model:\n",
    "        print(\"LLMが初期化されていません。要因生成をスキップします。\")\n",
    "        return []\n",
    "\n",
    "    # Geminiへの指示。JSON形式での出力を強調。\n",
    "    json_instruction_prompt = f\"\"\"\n",
    "あなたは、与えられたトピックや要因について、関連性の高い下位要因をマインドマップ形式で提案する専門家アシスタントです。\n",
    "提案する下位要因は、簡潔で具体的な名称にしてください。\n",
    "出力は、必ず下記のJSONスキーマに準拠した有効なJSONオブジェクト文字列のみとしてください。説明や前置き、後書きは一切含めないでください。\n",
    "\n",
    "JSONスキーマ:\n",
    "{{\n",
    "  \"factors\": [\"<提案する要因1>\", \"<提案する要因2>\", ..., \"<提案する要因{max_children}>\"]\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "    prompt_parts = [f\"マインドマップ全体の主題は「{context_problem}」です。\"]\n",
    "    if viewpoint_context and viewpoint_context != parent_factor:\n",
    "        prompt_parts.append(f\"現在フォーカスしている主要な観点は「{viewpoint_context}」です。\")\n",
    "    prompt_parts.append(f\"親要素は「{parent_factor}」です。\")\n",
    "    prompt_parts.append(f\"この親要素から展開される、第{target_depth}層に該当する具体的な下位要因を{max_children}個程度提案してください。（マインドマップ全体の最大層数は{max_depth_of_mindmap}層です）\")\n",
    "    \n",
    "    user_request_text = \"\\n\".join(prompt_parts)\n",
    "\n",
    "    # LangChainのプロンプトテンプレートと出力パーサーを使用\n",
    "    # SystemMessageはGeminiではHumanMessageに統合されることが多いので、指示はHumanMessageに含める\n",
    "    # または、ChatPromptTemplateの system メッセージを利用\n",
    "    prompt_template = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", json_instruction_prompt), # GeminiがSystemを解釈する場合\n",
    "        (\"human\", user_request_text)\n",
    "    ])\n",
    "    \n",
    "    parser = JsonOutputParser()\n",
    "    chain = prompt_template | llm_model | parser\n",
    "\n",
    "    # print(f\"\\n--- Generating factors for '{parent_factor}' (Target Depth: {target_depth}) using Gemini ---\")\n",
    "    # print(f\"Combined Prompt for LLM:\\nSystem: {json_instruction_prompt}\\nHuman: {user_request_text}\\n--------------------\")\n",
    "\n",
    "    try:\n",
    "        # invokeの入力はテンプレートの変数に合わせる必要があるが、ここでは直接文字列を渡しているので\n",
    "        # ChatPromptTemplate.from_template を使うか、messages を直接 invoke に渡す\n",
    "        # ここではテンプレートを使わず直接メッセージリストを作成する方がシンプルかもしれない\n",
    "\n",
    "        # 修正: ChatPromptTemplateの入力変数を使うか、メッセージリストを直接作成\n",
    "        # 以下はメッセージリストを直接作成する例\n",
    "        messages = [\n",
    "            SystemMessage(content=json_instruction_prompt), # Geminiの挙動によってはHumanに含める\n",
    "            HumanMessage(content=user_request_text)\n",
    "        ]\n",
    "        # response_from_llm = llm_model.invoke(messages) # このままだと Message オブジェクトが返る\n",
    "        # parsed_result = parser.parse(response_from_llm.content) # そのcontentをパース\n",
    "\n",
    "        # Chainを使う場合 (ChatPromptTemplate.from_messages で定義したテンプレートと連携)\n",
    "        # 入力は辞書形式で、テンプレート内のプレースホルダに対応させる\n",
    "        # この関数ではプレースホルダを使っていないため、invokeに直接メッセージリストを渡すのが適切\n",
    "        \n",
    "        # 修正案: invokeに直接メッセージリストを渡す\n",
    "        full_prompt_messages = [\n",
    "             # GeminiはSystemプロンプトを特別扱いしないことがあるため、指示を最初のUserターンに含める\n",
    "             HumanMessage(content=json_instruction_prompt + \"\\n\\n\" + user_request_text)\n",
    "        ]\n",
    "        \n",
    "        # もしSystemMessageを使いたい場合\n",
    "        # full_prompt_messages = [\n",
    "        #    SystemMessage(content=json_instruction_prompt), # これがHumanに変換されることを期待\n",
    "        #    HumanMessage(content=user_request_text)\n",
    "        # ]\n",
    "\n",
    "        # llm.invoke の結果を直接パースする\n",
    "        raw_response = llm_model.invoke(full_prompt_messages)\n",
    "        # print(f\"Gemini Raw Response content: {raw_response.content}\")\n",
    "        \n",
    "        parsed_result = parser.parse(raw_response.content)\n",
    "        # print(f\"Parsed Result: {parsed_result}\")\n",
    "\n",
    "        factors = parsed_result.get(\"factors\", [])\n",
    "        \n",
    "        if not isinstance(factors, list) or not all(isinstance(f, str) for f in factors):\n",
    "            print(f\"警告: Geminiからの応答が期待するリスト形式ではありませんでした。 Response: {factors}\")\n",
    "            return []\n",
    "            \n",
    "        return factors[:max_children] # 念のため個数制限\n",
    "\n",
    "    except json.JSONDecodeError as e:\n",
    "        # parser.parseで失敗した場合もこちらに来る可能性がある\n",
    "        print(f\"エラー: Geminiからの応答のJSONパースに失敗しました。Raw response: '{raw_response.content if 'raw_response' in locals() else 'N/A'}', Error: {e}\")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        # LangChainの呼び出しエラーなども含む\n",
    "        print(f\"エラー: LangChain/Gemini API呼び出し中にエラーが発生しました ({parent_factor}): {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "852d405d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# この関数は、呼び出す generate_factors_... 関数名と、渡すLLMインスタンスが変わる以外は\n",
    "# 前回のOpenAI版と同じロジックで動作します。\n",
    "# 関数名は get_children_recursively_gemini などに変更するとより明確になります。\n",
    "\n",
    "def get_children_recursively_gemini(\n",
    "    parent_factor_name: str,\n",
    "    context_problem: str,\n",
    "    current_child_depth: int,\n",
    "    max_mindmap_depth: int,\n",
    "    llm_instance, # LangChain LLM instance\n",
    "    viewpoint_context: str = None,\n",
    "    max_children_to_generate: int = 3\n",
    ") -> List[dict]:\n",
    "    \"\"\"\n",
    "    指定された親要素の子要素をGemini APIを使って再帰的に生成し、\n",
    "    子要素のリスト (各要素はノード辞書 {\"name\": \"...\", \"children\": [...]}) を返す。\n",
    "    \"\"\"\n",
    "    if current_child_depth > max_mindmap_depth:\n",
    "        return []\n",
    "\n",
    "    # Gemini APIを呼び出して子要素の名前リストを取得\n",
    "    child_factor_names = generate_factors_from_langchain_gemini( # ここをGemini用の関数に変更\n",
    "        parent_factor=parent_factor_name,\n",
    "        context_problem=context_problem,\n",
    "        target_depth=current_child_depth,\n",
    "        max_depth_of_mindmap=max_mindmap_depth,\n",
    "        llm_model=llm_instance, # LLMインスタンスを渡す\n",
    "        viewpoint_context=viewpoint_context,\n",
    "        max_children=max_children_to_generate\n",
    "    )\n",
    "\n",
    "    if not child_factor_names:\n",
    "        return []\n",
    "\n",
    "    children_nodes = []\n",
    "    indent_prefix = \"  \" * (current_child_depth -1) \n",
    "\n",
    "    for child_name in child_factor_names:\n",
    "        print(f\"{indent_prefix}- 要素 '{child_name}' (第{current_child_depth}層) の下位要素を探索中 (Gemini)...\")\n",
    "        \n",
    "        grandchildren_nodes = get_children_recursively_gemini( # 再帰呼び出しもこの関数自体\n",
    "            parent_factor_name=child_name,\n",
    "            context_problem=context_problem,\n",
    "            current_child_depth=current_child_depth + 1,\n",
    "            max_mindmap_depth=max_mindmap_depth,\n",
    "            llm_instance=llm_instance, # LLMインスタンスを引き継ぐ\n",
    "            viewpoint_context=viewpoint_context,\n",
    "            max_children_to_generate=max_children_to_generate\n",
    "        )\n",
    "        node = {\"name\": child_name}\n",
    "        if grandchildren_nodes:\n",
    "            node[\"children\"] = grandchildren_nodes\n",
    "        children_nodes.append(node)\n",
    "    \n",
    "    return children_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8737b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# このセルは前回のスクリプトから変更ありません。\n",
    "def convert_node_to_markdown_lines(node, list_depth=0):\n",
    "    \"\"\"\n",
    "    単一のノードとその子をMarkdownのリスト項目に変換する (再帰的)。\n",
    "    \"\"\"\n",
    "    lines = []\n",
    "    indent = \"  \" * list_depth\n",
    "    lines.append(f\"{indent}- {node['name']}\")\n",
    "    \n",
    "    if \"children\" in node and node[\"children\"]:\n",
    "        for child_node in node[\"children\"]:\n",
    "            lines.extend(convert_node_to_markdown_lines(child_node, list_depth + 1))\n",
    "    return lines\n",
    "\n",
    "def mindmap_to_markdown_string(mindmap_data):\n",
    "    \"\"\"\n",
    "    完全なマインドマップデータ構造を1つのMarkdown文字列に変換する。\n",
    "    \"\"\"\n",
    "    if not mindmap_data or \"name\" not in mindmap_data:\n",
    "        return \"# マインドマップの生成に失敗しました\"\n",
    "\n",
    "    markdown_lines = [f\"# {mindmap_data['name']}\\n\"]\n",
    "    \n",
    "    if \"children\" in mindmap_data and mindmap_data[\"children\"]:\n",
    "        for child_node in mindmap_data[\"children\"]:\n",
    "            markdown_lines.extend(convert_node_to_markdown_lines(child_node, list_depth=0))\n",
    "            markdown_lines.append(\"\") \n",
    "    \n",
    "    return \"\\n\".join(markdown_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7a48b79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- マインドマップ生成開始 (Gemini) ---\n",
      "指定された観点に基づいて第2層を構築します: ['製造プロセス', '材料', '使用環境']\n",
      "\n",
      "処理中の観点: '製造プロセス' (第2層)\n",
      "エラー: LangChain/Gemini API呼び出し中にエラーが発生しました (製造プロセス): name 'SystemMessage' is not defined\n",
      "\n",
      "処理中の観点: '材料' (第2層)\n",
      "エラー: LangChain/Gemini API呼び出し中にエラーが発生しました (材料): name 'SystemMessage' is not defined\n",
      "\n",
      "処理中の観点: '使用環境' (第2層)\n",
      "エラー: LangChain/Gemini API呼び出し中にエラーが発生しました (使用環境): name 'SystemMessage' is not defined\n",
      "\n",
      "--- マインドマップ生成完了 (Gemini) ---\n",
      "\n",
      "マインドマップがMarkdownファイル '導電性高分子コンデンサの信頼性悪化_mindmap_gemini.md' に保存されました。\n",
      "\n",
      "--- 生成されたMarkdown (プレビュー) ---\n",
      "# 導電性高分子コンデンサの信頼性悪化\n",
      "\n",
      "- 製造プロセス\n",
      "\n",
      "- 材料\n",
      "\n",
      "- 使用環境\n",
      "\n",
      "------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# LLMクライアントが正常に初期化されている場合のみ実行\n",
    "if llm: \n",
    "    print(\"\\n--- マインドマップ生成開始 (Gemini) ---\")\n",
    "    mindmap_data_structure = {\"name\": PROBLEM_TEXT, \"children\": []}\n",
    "\n",
    "    if MAX_DEPTH < 1:\n",
    "        print(\"エラー: 層数は1以上である必要があります。\")\n",
    "    elif MAX_DEPTH == 1:\n",
    "        print(f\"第1層 ({PROBLEM_TEXT}) のみのマインドマップです。\")\n",
    "    else: # MAX_DEPTH >= 2\n",
    "        if INITIAL_VIEWPOINTS:\n",
    "            print(f\"指定された観点に基づいて第2層を構築します: {INITIAL_VIEWPOINTS}\")\n",
    "            for viewpoint_name in INITIAL_VIEWPOINTS:\n",
    "                print(f\"\\n処理中の観点: '{viewpoint_name}' (第2層)\")\n",
    "                children_of_viewpoint = get_children_recursively_gemini( # Gemini版関数を呼び出し\n",
    "                    parent_factor_name=viewpoint_name,\n",
    "                    context_problem=PROBLEM_TEXT,\n",
    "                    current_child_depth=3,\n",
    "                    max_mindmap_depth=MAX_DEPTH,\n",
    "                    llm_instance=llm, # 初期化されたLLMインスタンス\n",
    "                    viewpoint_context=viewpoint_name,\n",
    "                    max_children_to_generate=MAX_CHILDREN_PER_NODE\n",
    "                )\n",
    "                viewpoint_node = {\"name\": viewpoint_name}\n",
    "                if children_of_viewpoint:\n",
    "                    viewpoint_node[\"children\"] = children_of_viewpoint\n",
    "                mindmap_data_structure[\"children\"].append(viewpoint_node)\n",
    "        else:\n",
    "            print(\"観点の指定がないため、問題の直下の要素 (第2層) を生成します。\")\n",
    "            direct_children = get_children_recursively_gemini( # Gemini版関数を呼び出し\n",
    "                parent_factor_name=PROBLEM_TEXT,\n",
    "                context_problem=PROBLEM_TEXT,\n",
    "                current_child_depth=2,\n",
    "                max_mindmap_depth=MAX_DEPTH,\n",
    "                llm_instance=llm, # 初期化されたLLMインスタンス\n",
    "                viewpoint_context=None,\n",
    "                max_children_to_generate=MAX_CHILDREN_PER_NODE\n",
    "            )\n",
    "            if direct_children:\n",
    "                mindmap_data_structure[\"children\"] = direct_children\n",
    "\n",
    "    print(\"\\n--- マインドマップ生成完了 (Gemini) ---\")\n",
    "\n",
    "    markdown_output_string = mindmap_to_markdown_string(mindmap_data_structure)\n",
    "\n",
    "    try:\n",
    "        with open(OUTPUT_FILENAME, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(markdown_output_string)\n",
    "        print(f\"\\nマインドマップがMarkdownファイル '{OUTPUT_FILENAME}' に保存されました。\")\n",
    "    except IOError as e:\n",
    "        print(f\"エラー: ファイル '{OUTPUT_FILENAME}' の書き込みに失敗しました: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"予期せぬエラーが発生しました: {e}\")\n",
    "\n",
    "    print(\"\\n--- 生成されたMarkdown (プレビュー) ---\")\n",
    "    print(markdown_output_string)\n",
    "    print(\"------------------------------------\")\n",
    "\n",
    "else:\n",
    "    print(\"LangChain LLM (Gemini) が初期化されていないため、マインドマップ生成処理をスキップしました。\")\n",
    "    print(\"セル1およびセル2の設定とAPIキーを確認してください。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8924001",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
