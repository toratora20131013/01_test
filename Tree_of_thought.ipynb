{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83af8e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json # 今回は直接使用しませんが、構造化データ連携の際に有用\n",
    "from dotenv import load_dotenv\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI # Google Geminiモデル用\n",
    "\n",
    "# .envファイルから環境変数を読み込む\n",
    "load_dotenv()\n",
    "\n",
    "print(\"ライブラリのインポートと環境変数のロードが完了しました。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29cdeb83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# APIキーの読み込み\n",
    "api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "\n",
    "if not api_key:\n",
    "    print(\"エラー: 環境変数 'GEMINI_API_KEY' が .env ファイルに設定されていません。\")\n",
    "    print(\".env ファイルを作成し、GEMINI_API_KEY=\\\"YOUR_KEY_HERE\\\" のように記述してください。\")\n",
    "    # このセル以降の実行を中断させるために例外を発生させるか、条件分岐で制御\n",
    "    raise ValueError(\"Gemini APIキーが設定されていません。処理を中断します。\")\n",
    "else:\n",
    "    print(\"Gemini APIキーが正常に読み込まれました。\")\n",
    "\n",
    "# LLMクライアントの初期化\n",
    "# 注意: 'gemini-2.0-flash' というモデル名は2025年5月時点で一般的でない可能性があります。\n",
    "# 利用可能なモデル名（例: 'gemini-1.5-flash-latest', 'gemini-pro'など）を適宜確認・変更してください。\n",
    "# エラーが出る場合は、モデル名をGoogle Cloudのドキュメントで確認してください。\n",
    "try:\n",
    "    llm = ChatGoogleGenerativeAI(\n",
    "        model=\"gemini-2.0-flash\", # ユーザー指定は \"gemini-2.0-flash\" でしたが、より一般的なモデル名に変更。必要に応じて調整してください。\n",
    "        google_api_key=api_key,\n",
    "        temperature=0, # ユーザー指定のデフォルトtemperature\n",
    "        # convert_system_message_to_human=True # 必要に応じてシステムメッセージの扱いを調整\n",
    "    )\n",
    "    print(f\"ChatGoogleGenerativeAIクライアントがモデル '{llm.model}' で初期化されました。\")\n",
    "except Exception as e:\n",
    "    print(f\"LLMクライアントの初期化中にエラーが発生しました: {e}\")\n",
    "    print(\"モデル名が正しいか、APIキーに十分な権限があるか確認してください。\")\n",
    "    llm = None # エラー時はllmをNoneに設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6801186",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_llm(prompt_text: str, temperature: float = None, max_tokens: int = None) -> str:\n",
    "    \"\"\"\n",
    "    初期化されたLLMクライアントを使用してプロンプトを実行し、応答を取得する関数。\n",
    "    temperature や max_tokens は呼び出し時に指定があれば、LLM初期化時の設定を上書き試行します。\n",
    "    \"\"\"\n",
    "    if llm is None:\n",
    "        return \"Error: LLMクライアントが初期化されていません。\"\n",
    "\n",
    "    try:\n",
    "        # LangchainのChatGoogleGenerativeAIはinvokeメソッドの引数で直接temperatureやmax_tokensを\n",
    "        # 簡単には上書きできない場合があります。\n",
    "        # 通常は初期化時に設定するか、GenerationConfigを使います。\n",
    "        # ここでは、初期化時の設定を基本としつつ、もし個別に設定したい場合は\n",
    "        # 新しいLLMインスタンスをその都度生成するか、より高度な設定方法を検討します。\n",
    "        # 今回はシンプルに、初期化時の設定で動作させます。\n",
    "        # もし個別の呼び出しでtemperatureやmax_output_tokensを変更したい場合は、以下のようにします:\n",
    "        \n",
    "        current_model_kwargs = llm.model_kwargs if llm.model_kwargs else {}\n",
    "        generation_config = {}\n",
    "        if temperature is not None:\n",
    "            generation_config['temperature'] = temperature\n",
    "        if max_tokens is not None:\n",
    "            generation_config['max_output_tokens'] = max_tokens\n",
    "        \n",
    "        if generation_config:\n",
    "            # invokeにgeneration_configを渡せるか確認 (LangChainのバージョンやモデルによる)\n",
    "            # もし直接渡せない場合は、llm.with_structured_outputやllm.bindなどで設定変更が必要な場合あり\n",
    "            # ここでは簡略化のため、初期化時の設定が優先されることを前提とします。\n",
    "            # ChatGoogleGenerativeAI の invoke に直接 generation_config を渡すのは標準的ではない。\n",
    "            # .bind() や .with_options() を使うのが一般的\n",
    "            \n",
    "            # invoke時に設定を適用する例 (ただし、これは ChatGoogleGenerativeAI の標準的な使い方とは異なる可能性あり)\n",
    "            # response = llm.invoke(prompt_text, generation_config=generation_config)\n",
    "            \n",
    "            # より確実なのは、新しい設定でインスタンスを一時的に作るか、bindを使う\n",
    "            # 例: llm.bind(generation_config=generation_config).invoke(prompt_text)\n",
    "            # ここでは簡単のため、初期化設定を使い、引数は無視する形で進めます。\n",
    "            # 必要であれば、より高度な設定方法を実装してください。\n",
    "            if temperature is not None and llm.temperature != temperature:\n",
    "                 print(f\"  call_llm: temperature引数({temperature})は現在無視され、初期化時の値({llm.temperature})が使用されます。\")\n",
    "            # max_tokensについては、ChatGoogleGenerativeAIの初期化では直接指定せず、\n",
    "            # generate_config内のmax_output_tokensで制御するため、ここでは特に触れません。\n",
    "\n",
    "        response = llm.invoke(prompt_text)\n",
    "        return response.content.strip()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"LLM呼び出し中にエラーが発生しました: {e}\")\n",
    "        return f\"Error: LLMからの応答取得に失敗しました。詳細: {e}\"\n",
    "\n",
    "# テスト呼び出し (任意)\n",
    "# test_prompt = \"こんにちは、今日の調子はどうですか？\"\n",
    "# test_response = call_llm(test_prompt)\n",
    "# print(f\"テストプロンプト: {test_prompt}\")\n",
    "# print(f\"テスト応答: {test_response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92621406",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThoughtNode:\n",
    "    \"\"\"思考の木における各ノードを表すクラス\"\"\"\n",
    "    def __init__(self, text: str, parent=None, evaluation: str = None, id_path: str =\"\"):\n",
    "        self.text = text.strip()\n",
    "        self.parent = parent\n",
    "        self.children: list[ThoughtNode] = []\n",
    "        self.evaluation = evaluation\n",
    "        self.id_path = id_path\n",
    "\n",
    "    def add_child(self, child_node):\n",
    "        self.children.append(child_node)\n",
    "\n",
    "    def __repr__(self, level=0) -> str:\n",
    "        eval_text = f\" (評価: {self.evaluation})\" if self.evaluation else \"\"\n",
    "        id_text = f\" (ID: {self.id_path})\" if self.id_path else \"\"\n",
    "        ret = \"\\t\" * level + repr(self.text) + eval_text + id_text + \"\\n\"\n",
    "        for child in self.children:\n",
    "            ret += child.__repr__(level + 1)\n",
    "        return ret\n",
    "\n",
    "print(\"ThoughtNode クラスが定義されました。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be78442",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_initial_thoughts(problem: str, num_thoughts: int = 3) -> list[str]:\n",
    "    \"\"\"与えられたお題に対して、初期の複数の思考（アイデア）を生成する\"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    お題: 「{problem}」\n",
    "\n",
    "    このお題について、{num_thoughts}個の異なる初期アイデアや思考の方向性をリスト形式で提案してください。\n",
    "    各アイデアは簡潔に記述し、それぞれ独立した行に記述してください。\n",
    "\n",
    "    例:\n",
    "    - アイデアA\n",
    "    - アイデアB\n",
    "    - アイデアC\n",
    "\n",
    "    あなたの提案:\n",
    "    \"\"\"\n",
    "    # LLM初期化時のtemperatureが0なので、ここではtemperatureを指定しない\n",
    "    response = call_llm(prompt_text=prompt, max_tokens=num_thoughts * 50)\n",
    "    if response.startswith(\"Error:\") : return [response]\n",
    "    thoughts = [line.strip().lstrip('- ').lstrip('* ') for line in response.split('\\n') if line.strip()]\n",
    "    return thoughts[:num_thoughts] if thoughts else [\"生成されたアイデアがありませんでした。\"]\n",
    "\n",
    "def expand_thought(thought_text: str, problem: str, current_path_str: str, num_children: int = 2) -> list[str]:\n",
    "    \"\"\"特定の思考をさらに深掘りし、サブアイデアや次のステップを生成する\"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    現在のお題: 「{problem}」\n",
    "    現在の思考の文脈 (パス: {current_path_str}): 「{thought_text}」\n",
    "\n",
    "    上記の思考「{thought_text}」をさらに具体的に展開し、{num_children}個の異なる詳細なサブアイデア、関連する問い、または次の具体的なアクションステップをリスト形式で提案してください。\n",
    "    各提案は簡潔に記述し、それぞれ独立した行に記述してください。\n",
    "\n",
    "    あなたの提案:\n",
    "    \"\"\"\n",
    "    response = call_llm(prompt_text=prompt, max_tokens=num_children * 70)\n",
    "    if response.startswith(\"Error:\") : return [response]\n",
    "    sub_thoughts = [line.strip().lstrip('- ').lstrip('* ') for line in response.split('\\n') if line.strip()]\n",
    "    return sub_thoughts[:num_children] if sub_thoughts else [\"生成されたサブアイデアがありませんでした。\"]\n",
    "\n",
    "def evaluate_thought_viability(thought_text: str, problem: str, current_path_str: str) -> str:\n",
    "    \"\"\"思考の有望性や妥当性をLLMに評価させる（簡易版）\"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    お題: 「{problem}」\n",
    "    現在の思考パス: {current_path_str}\n",
    "    検討中の思考: 「{thought_text}」\n",
    "\n",
    "    この思考「{thought_text}」は、お題「{problem}」の解決や探求において、どの程度有望ですか？\n",
    "    評価結果を「有望」「中立」「低リスク」「高リスク」「要改善」「興味深い」などの短いキーワードで述べてください。\n",
    "    可能であれば、その簡単な理由も付け加えてください。\n",
    "\n",
    "    評価: [キーワード] (理由: [簡単な理由])\n",
    "    \"\"\"\n",
    "    # 評価なので、もしtemperatureを調整したければここで指定（ただし上記call_llmの実装による）\n",
    "    response = call_llm(prompt_text=prompt, temperature=0.2, max_tokens=100) # 評価なので低めのtemperatureを試みる\n",
    "    if response.startswith(\"Error:\"): return \"評価エラー\"\n",
    "    return response\n",
    "\n",
    "print(\"ヘルパー関数 (generate_initial_thoughts, expand_thought, evaluate_thought_viability) が定義されました。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76321cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tree_of_thoughts_generator(\n",
    "    initial_problem: str,\n",
    "    max_depth: int = 2,\n",
    "    initial_thoughts_count: int = 3,\n",
    "    children_per_node: int = 2,\n",
    "    use_evaluation: bool = False\n",
    ") -> ThoughtNode:\n",
    "    \"\"\"Tree of Thoughtsを生成するメイン関数\"\"\"\n",
    "    if llm is None:\n",
    "        print(\"LLMクライアントが初期化されていないため、Tree of Thoughtsを生成できません。\")\n",
    "        return ThoughtNode(\"Error: LLM not initialized.\")\n",
    "\n",
    "    root_node = ThoughtNode(initial_problem, id_path=\"0\")\n",
    "    root_node.evaluation = \"初期お題\"\n",
    "\n",
    "    queue: list[tuple[ThoughtNode, int]] = [(root_node, 0)]\n",
    "    node_id_counters: dict[str, int] = {\"0\": 0}\n",
    "    visited_texts: set[str] = {initial_problem.lower()}\n",
    "    processed_nodes_count = 0\n",
    "\n",
    "    while queue:\n",
    "        current_node, depth = queue.pop(0)\n",
    "        processed_nodes_count += 1\n",
    "        print(f\"\\nProcessing node (depth {depth}, ID: {current_node.id_path}): {current_node.text[:80]}...\")\n",
    "\n",
    "        if depth >= max_depth:\n",
    "            print(\"  Max depth reached for this branch.\")\n",
    "            continue\n",
    "\n",
    "        generated_ideas: list[str] = []\n",
    "        if depth == 0:\n",
    "            print(f\"  Generating initial {initial_thoughts_count} thoughts for the problem...\")\n",
    "            generated_ideas = generate_initial_thoughts(current_node.text, initial_thoughts_count)\n",
    "        else:\n",
    "            print(f\"  Expanding thought into {children_per_node} sub-thoughts...\")\n",
    "            generated_ideas = expand_thought(\n",
    "                current_node.text,\n",
    "                initial_problem,\n",
    "                current_node.id_path,\n",
    "                children_per_node\n",
    "            )\n",
    "        \n",
    "        print(f\"    Generated {len(generated_ideas)} ideas: {generated_ideas}\")\n",
    "\n",
    "        parent_id_path = current_node.id_path\n",
    "        if parent_id_path not in node_id_counters:\n",
    "            node_id_counters[parent_id_path] = 0\n",
    "\n",
    "        for idea_text in generated_ideas:\n",
    "            if not idea_text or idea_text.startswith(\"Error:\") or idea_text.lower() in visited_texts:\n",
    "                print(f\"    Skipping duplicate or invalid idea: {idea_text[:80]}\")\n",
    "                continue\n",
    "            visited_texts.add(idea_text.lower())\n",
    "\n",
    "            child_id_num = node_id_counters[parent_id_path]\n",
    "            node_id_counters[parent_id_path] += 1\n",
    "            new_id_path = f\"{parent_id_path}-{child_id_num}\"\n",
    "\n",
    "            evaluation_result = None\n",
    "            if use_evaluation and depth < max_depth :\n",
    "                print(f\"    Evaluating sub-thought (ID: {new_id_path}): {idea_text[:80]}...\")\n",
    "                evaluation_result = evaluate_thought_viability(idea_text, initial_problem, new_id_path)\n",
    "                print(f\"      Evaluation: {evaluation_result}\")\n",
    "\n",
    "            child_node = ThoughtNode(idea_text, parent=current_node, evaluation=evaluation_result, id_path=new_id_path)\n",
    "            current_node.add_child(child_node)\n",
    "            \n",
    "            if depth + 1 < max_depth:\n",
    "                 queue.append((child_node, depth + 1))\n",
    "            elif depth + 1 == max_depth:\n",
    "                 print(f\"  Reached max depth for child: {child_node.text[:80]}\")\n",
    "\n",
    "    print(f\"\\nTree generation complete. Processed {processed_nodes_count} nodes.\")\n",
    "    return root_node\n",
    "\n",
    "print(\"tree_of_thoughts_generator 関数が定義されました。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13856208",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_to_markdown_mindmap(node: ThoughtNode, level: int = 0, include_eval: bool = True) -> str:\n",
    "    \"\"\"思考の木をMarkdownのマインドマップ形式（Markmap互換）に変換する\"\"\"\n",
    "    markdown = \"\"\n",
    "    indent = \"  \" * level \n",
    "    prefix = \"# \" if level == 0 else \"- \"\n",
    "    text_to_display = node.text.replace(\"\\n\", \" \") # 複数行は1行に\n",
    "    if include_eval and node.evaluation:\n",
    "        eval_short = node.evaluation.split('\\n')[0] # 評価が複数行なら最初の行のみ\n",
    "        text_to_display += f\"  <small>(評価: {eval_short})</small>\" # Markmapで小さく表示\n",
    "\n",
    "    markdown += f\"{indent}{prefix}{text_to_display}\\n\"\n",
    "    for child in node.children:\n",
    "        markdown += format_to_markdown_mindmap(child, level + 1, include_eval)\n",
    "    return markdown\n",
    "\n",
    "print(\"format_to_markdown_mindmap 関数が定義されました。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff14ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_final_answer_from_tree(root_node: ThoughtNode, initial_problem: str, max_paths_to_consider: int = 5) -> str:\n",
    "    \"\"\"生成された思考の木から有望な経路を抽出し、LLMに最終的な結論を生成させる\"\"\"\n",
    "    if llm is None:\n",
    "        return \"Error: LLMクライアントが初期化されていないため、最終結論を生成できません。\"\n",
    "        \n",
    "    paths: list[list[ThoughtNode]] = []\n",
    "    def find_paths_to_leaves(node: ThoughtNode, current_path: list[ThoughtNode]):\n",
    "        current_path.append(node)\n",
    "        if not node.children: # 葉ノード\n",
    "            paths.append(list(current_path)) # パスのコピーを保存\n",
    "        else:\n",
    "            for child in node.children:\n",
    "                find_paths_to_leaves(child, current_path)\n",
    "        current_path.pop() # バックトラック\n",
    "\n",
    "    find_paths_to_leaves(root_node, [])\n",
    "    if not paths: return \"思考の木から有効な思考経路が見つかりませんでした。\"\n",
    "\n",
    "    path_texts_for_llm = []\n",
    "    # 経路を評価や深さでソートするなど、より高度な選択も可能\n",
    "    for path in paths[:max_paths_to_consider]:\n",
    "        # ルートノード（お題自身）はパス説明に含めないことが多い\n",
    "        path_str = \" -> \".join([node.text for node in path[1:]]) # path[0]はルート\n",
    "        if path_str: # 空のパスは含めない\n",
    "             path_texts_for_llm.append(path_str)\n",
    "\n",
    "    if not path_texts_for_llm: return \"最終結論を生成するための有効な思考経路が抽出できませんでした。\"\n",
    "\n",
    "    context_for_final_answer = \"以下は、お題「\" + initial_problem + \"」についてTree of Thoughtsの手法で展開された主要な思考の経路です。\\n\\n\"\n",
    "    for i, path_text in enumerate(path_texts_for_llm):\n",
    "        context_for_final_answer += f\"経路 {i+1}: {path_text}\\n\"\n",
    "    \n",
    "    context_for_final_answer += \"\\nこれらの思考の経路全体を踏まえて、お題「\" + initial_problem + \"」に対する最も包括的で洞察に富んだ最終的な結論、または複数の選択肢がある場合はそれらを整理した提案をまとめてください。\"\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    お題: {initial_problem}\n",
    "\n",
    "    提供された思考の木（Tree of Thoughts）から抽出された主要な思考経路:\n",
    "    {context_for_final_answer}\n",
    "\n",
    "    上記の分析結果に基づき、お題に対する最終的な結論や提案を、構造的かつ分かりやすくまとめてください。\n",
    "    複数のアイデアがある場合は、それらを統合したり、比較したり、優先順位を示したりすることも検討してください。\n",
    "    結論は、具体的なアクションや次のステップに繋がるような形で提示することが望ましいです。\n",
    "    \"\"\"\n",
    "    print(\"\\n--- LLMに最終結論の生成を依頼中 ---\")\n",
    "    # 結論生成時はtemperatureを少し上げて多様性を出すことも検討できる\n",
    "    final_answer = call_llm(prompt_text=prompt, temperature=0.5, max_tokens=1500)\n",
    "    return final_answer\n",
    "\n",
    "print(\"get_final_answer_from_tree 関数が定義されました。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3362ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\": # Jupyter Notebookではこのブロックは通常直接実行されないが、\n",
    "                           # スクリプトとして実行された場合のために残しておいても良い\n",
    "    print(\"メイン実行ロジックを開始します (Jupyter Notebookではこのセルを直接実行してください)。\")\n",
    "\n",
    "# --- Jupyter Notebookで直接実行する部分 ---\n",
    "if llm is None:\n",
    "    print(\"LLMクライアントが初期化されていないため、処理を実行できません。セル2を再確認してください。\")\n",
    "else:\n",
    "    # ▼▼▼ お題を直接ここに記述してください ▼▼▼\n",
    "    initial_problem_prompt = \"機械学習の基礎\"\n",
    "    # 例: \"再生可能エネルギーの普及を加速するための斬新な政策提言\"\n",
    "    # 例: \"2030年までに日本の食品ロスを半減させるための実行可能な戦略\"\n",
    "    # ▲▲▲ お題の記述はここまで ▲▲▲\n",
    "\n",
    "    print(f\"設定されたお題: 「{initial_problem_prompt}」\")\n",
    "\n",
    "    if not initial_problem_prompt.strip():\n",
    "        print(\"エラー: お題が設定されていません。上記の initial_problem_prompt にお題を記述してください。\")\n",
    "        # 必要に応じて処理を中断\n",
    "        # exit() # Jupyter Notebookではexit()はカーネルを停止させる可能性があるので注意\n",
    "    else:\n",
    "        # ToTのパラメータ設定\n",
    "        max_depth_setting = 2        # 木の深さ (0はお題自身。1で初期思考、2でサブ思考)\n",
    "        initial_thoughts_count = 3   # 初期に生成する思考の数\n",
    "        children_per_node_count = 2  # 各思考から展開するサブ思考の数\n",
    "        enable_evaluation = False    # 各思考の評価を行うか (TrueにするとLLMコール数が増加)\n",
    "\n",
    "        print(f\"\\nお題「{initial_problem_prompt}」についてTree of Thoughtsを開始します...\")\n",
    "        print(f\"設定: 深さ={max_depth_setting}, 初期思考数={initial_thoughts_count}, 各ノードの子思考数={children_per_node_count}, 評価={enable_evaluation}\\n\")\n",
    "\n",
    "        # Tree of Thoughtsを生成\n",
    "        thought_tree_root = tree_of_thoughts_generator(\n",
    "            initial_problem_prompt,\n",
    "            max_depth=max_depth_setting,\n",
    "            initial_thoughts_count=initial_thoughts_count,\n",
    "            children_per_node=children_per_node_count,\n",
    "            use_evaluation=enable_evaluation\n",
    "        )\n",
    "\n",
    "        if thought_tree_root.text.startswith(\"Error:\"):\n",
    "            print(f\"Tree of Thoughtsの生成に失敗しました: {thought_tree_root.text}\")\n",
    "        else:\n",
    "            print(\"\\n\\n--- 生成された思考の木 (内部構造プレビュー) ---\")\n",
    "            for i, child in enumerate(thought_tree_root.children):\n",
    "                print(f\"  初期思考 {i+1}: {child.text}\")\n",
    "                for j, grandchild in enumerate(child.children):\n",
    "                    print(f\"    サブ思考 {i+1}-{j+1}: {grandchild.text}\")\n",
    "\n",
    "            # Markdownマインドマップ形式に変換\n",
    "            markdown_output = format_to_markdown_mindmap(thought_tree_root, include_eval=enable_evaluation)\n",
    "\n",
    "            print(\"\\n\\n--- マインドマップ用Markdown出力 ---\")\n",
    "            print(markdown_output) # Notebookのセルに出力\n",
    "\n",
    "            # Markdownファイルを保存\n",
    "            filename_md = \"thought_tree_mindmap.md\"\n",
    "            try:\n",
    "                with open(filename_md, \"w\", encoding=\"utf-8\") as f:\n",
    "                    f.write(markdown_output)\n",
    "                print(f\"\\nマインドマップが '{filename_md}' に保存されました。\")\n",
    "                print(\"このファイルをMarkmap (例: https://markmap.js.org/repl ) やVSCodeのMarkdown Preview Enhanced拡張機能などで開くと、視覚的なマインドマップとして表示できます。\")\n",
    "            except Exception as e:\n",
    "                print(f\"Markdownファイルの保存中にエラーが発生しました: {e}\")\n",
    "\n",
    "            # 生成された思考の木に基づいて最終的な結論をLLMに生成させる\n",
    "            print(\"\\n\\n--- Tree of Thoughtsに基づく最終結論の生成 ---\")\n",
    "            final_conclusion = get_final_answer_from_tree(thought_tree_root, initial_problem_prompt)\n",
    "            \n",
    "            print(\"\\n\\n--- 最終結論 ---\")\n",
    "            print(final_conclusion) # Notebookのセルに出力\n",
    "\n",
    "            # 最終結論をファイルに保存\n",
    "            filename_conclusion = \"final_conclusion.txt\"\n",
    "            try:\n",
    "                with open(filename_conclusion, \"w\", encoding=\"utf-8\") as f:\n",
    "                    f.write(f\"お題: {initial_problem_prompt}\\n\\n\")\n",
    "                    f.write(\"生成された思考の木 (Markdownマインドマップ形式):\\n\")\n",
    "                    f.write(markdown_output)\n",
    "                    f.write(\"\\n\\n---\\n\\n\")\n",
    "                    f.write(\"最終結論:\\n\")\n",
    "                    f.write(final_conclusion)\n",
    "                print(f\"\\n最終結論と思考プロセスが '{filename_conclusion}' に保存されました。\")\n",
    "            except Exception as e:\n",
    "                print(f\"最終結論ファイルの保存中にエラーが発生しました: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9181bd2b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
